{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. CONFIGURACIÓN GENERAL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "LR = 1e-3\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. CARGA DE DATOS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. DEFINICIÓN DE MODELOS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size=784,   # 28 x 28\n",
    "                 hidden_layers=[128, 64],\n",
    "                 output_size=10,\n",
    "                 activation='relu',\n",
    "                 dropout=0.0):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        in_features = input_size\n",
    "        \n",
    "        if activation.lower() == 'relu':\n",
    "            activation_fn = nn.ReLU()\n",
    "        elif activation.lower() == 'tanh':\n",
    "            activation_fn = nn.Tanh()\n",
    "        else:\n",
    "            raise ValueError(\"Error en la función de activación.\")\n",
    "\n",
    "        for h in hidden_layers:\n",
    "            layers.append(nn.Linear(in_features, h))\n",
    "            layers.append(activation_fn)\n",
    "            if dropout > 0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            in_features = h\n",
    "        \n",
    "        layers.append(nn.Linear(in_features, output_size))\n",
    "        \n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.net(x)\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_filters=[32, 64],\n",
    "                 fc_sizes=[128],\n",
    "                 output_size=10,\n",
    "                 dropout=0.0):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        layers = []\n",
    "        \n",
    "        in_channels = 1\n",
    "        for nf in num_filters:\n",
    "            layers.append(nn.Conv2d(in_channels, nf, kernel_size=3, padding=1))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            if dropout > 0:\n",
    "                layers.append(nn.Dropout2d(dropout))\n",
    "            in_channels = nf\n",
    "        \n",
    "        self.conv = nn.Sequential(*layers)\n",
    "        self.flatten_size = num_filters[-1] * (28 // 2**len(num_filters))**2\n",
    "        \n",
    "        fc_layers = []\n",
    "        in_features = self.flatten_size\n",
    "        for fs in fc_sizes:\n",
    "            fc_layers.append(nn.Linear(in_features, fs))\n",
    "            fc_layers.append(nn.ReLU())\n",
    "            if dropout > 0:\n",
    "                fc_layers.append(nn.Dropout(dropout))\n",
    "            in_features = fs\n",
    "        \n",
    "        fc_layers.append(nn.Linear(in_features, output_size))\n",
    "        \n",
    "        self.fc = nn.Sequential(*fc_layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1) \n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. FUNCIONES DE ENTRENAMIENTO Y VALIDACIÓN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in dataloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5. EXPERIMENTACIÓN CON MLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mlp_experiment(hidden_layers_list, activations, dropouts, optimizers, lrs, batch_sizes, num_epochs):\n",
    "    best_acc = 0.0\n",
    "    best_config = None\n",
    "    \n",
    "    for hl in hidden_layers_list:\n",
    "        for act in activations:\n",
    "            for dr in dropouts:\n",
    "                for opt_name in optimizers:\n",
    "                    for lr_ in lrs:\n",
    "                        for bs in batch_sizes:\n",
    "                            \n",
    "                            train_loader_tmp = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "                            test_loader_tmp = DataLoader(test_dataset, batch_size=bs, shuffle=False)\n",
    "                            \n",
    "                            model = MLP(hidden_layers=hl, activation=act, dropout=dr).to(device)\n",
    "                            criterion = nn.CrossEntropyLoss()\n",
    "                            \n",
    "                            if opt_name == 'sgd':\n",
    "                                optimizer = optim.SGD(model.parameters(), lr=lr_, momentum=0.9)\n",
    "                            elif opt_name == 'adam':\n",
    "                                optimizer = optim.Adam(model.parameters(), lr=lr_)\n",
    "                            else:\n",
    "                                raise ValueError(\"Optimizador no soportado.\")\n",
    "                            \n",
    "                            print(f\"\\nEntrenando MLP con config: {hl}, act={act}, dropout={dr}, \"\n",
    "                                  f\"opt={opt_name}, lr={lr_}, batch_size={bs}\")\n",
    "                            \n",
    "                            start_time = time.time()\n",
    "                            for epoch in range(num_epochs):\n",
    "                                train_loss, train_acc = train_one_epoch(model, train_loader_tmp, criterion, optimizer)\n",
    "                                val_loss, val_acc = evaluate(model, test_loader_tmp, criterion)\n",
    "                                print(f\"[Epoch {epoch+1}/{num_epochs}] \"\n",
    "                                      f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "                                      f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "                            \n",
    "                            elapsed = time.time() - start_time\n",
    "                            print(f\"Tiempo de entrenamiento: {elapsed:.2f} seg\\n\")\n",
    "                            \n",
    "                            if val_acc > best_acc:\n",
    "                                best_acc = val_acc\n",
    "                                best_config = (hl, act, dr, opt_name, lr_, bs, val_acc, elapsed)\n",
    "    \n",
    "    print(\"Mejor configuración MLP encontrada:\")\n",
    "    if best_config is not None:\n",
    "        hl, act, dr, opt_name, lr_, bs, val_acc, time_ = best_config\n",
    "        print(f\"  hidden_layers={hl}, activation={act}, dropout={dr}, optimizer={opt_name}, \"\n",
    "              f\"  lr={lr_}, batch_size={bs}, best_val_acc={val_acc:.4f}, time={time_:.2f}s\")\n",
    "    else:\n",
    "        print(\"  No se encontraron configuraciones válidas (revisa tus parámetros).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **6. EXPERIMENTACIÓN CON CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cnn_experiment(filters_list, fc_sizes_list, dropouts, optimizers, lrs, batch_sizes, num_epochs):\n",
    "    best_acc = 0.0\n",
    "    best_config = None\n",
    "    \n",
    "    for filt in filters_list:\n",
    "        for fc in fc_sizes_list:\n",
    "            for dr in dropouts:\n",
    "                for opt_name in optimizers:\n",
    "                    for lr_ in lrs:\n",
    "                        for bs in batch_sizes:\n",
    "                            \n",
    "                            train_loader_tmp = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "                            test_loader_tmp = DataLoader(test_dataset, batch_size=bs, shuffle=False)\n",
    "                            \n",
    "                            model = SimpleCNN(num_filters=filt, fc_sizes=fc, dropout=dr).to(device)\n",
    "                            criterion = nn.CrossEntropyLoss()\n",
    "                            \n",
    "                            if opt_name == 'sgd':\n",
    "                                optimizer = optim.SGD(model.parameters(), lr=lr_, momentum=0.9)\n",
    "                            elif opt_name == 'adam':\n",
    "                                optimizer = optim.Adam(model.parameters(), lr=lr_)\n",
    "                            else:\n",
    "                                raise ValueError(\"Optimizador no soportado.\")\n",
    "                            \n",
    "                            print(f\"\\nEntrenando CNN con config: filtros={filt}, fc={fc}, dropout={dr}, \"\n",
    "                                  f\"opt={opt_name}, lr={lr_}, batch_size={bs}\")\n",
    "                            \n",
    "                            start_time = time.time()\n",
    "                            for epoch in range(num_epochs):\n",
    "                                train_loss, train_acc = train_one_epoch(model, train_loader_tmp, criterion, optimizer)\n",
    "                                val_loss, val_acc = evaluate(model, test_loader_tmp, criterion)\n",
    "                                print(f\"[Epoch {epoch+1}/{num_epochs}] \"\n",
    "                                      f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "                                      f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "                            \n",
    "                            elapsed = time.time() - start_time\n",
    "                            print(f\"Tiempo de entrenamiento: {elapsed:.2f} seg\\n\")\n",
    "                            \n",
    "                            if val_acc > best_acc:\n",
    "                                best_acc = val_acc\n",
    "                                best_config = (filt, fc, dr, opt_name, lr_, bs, val_acc, elapsed)\n",
    "    \n",
    "    print(\"Mejor configuración CNN encontrada:\")\n",
    "    if best_config is not None:\n",
    "        filt, fc, dr, opt_name, lr_, bs, val_acc, time_ = best_config\n",
    "        print(f\"  filtros={filt}, fc={fc}, dropout={dr}, optimizer={opt_name}, \"\n",
    "              f\"  lr={lr_}, batch_size={bs}, best_val_acc={val_acc:.4f}, time={time_:.2f}s\")\n",
    "    else:\n",
    "        print(\"  No se encontraron configuraciones válidas (revisa tus parámetros).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **7. COMPARACIÓN DE RESULTADOS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entrenando MLP con config: [128, 64], act=relu, dropout=0.0, opt=sgd, lr=0.001, batch_size=64\n",
      "[Epoch 1/3] Train Loss: 0.8595, Train Acc: 0.7761 | Val Loss: 0.3468, Val Acc: 0.9009\n",
      "[Epoch 2/3] Train Loss: 0.3152, Train Acc: 0.9091 | Val Loss: 0.2679, Val Acc: 0.9233\n",
      "[Epoch 3/3] Train Loss: 0.2577, Train Acc: 0.9254 | Val Loss: 0.2284, Val Acc: 0.9334\n",
      "Tiempo de entrenamiento: 78.87 seg\n",
      "\n",
      "\n",
      "Entrenando MLP con config: [128, 64], act=relu, dropout=0.0, opt=adam, lr=0.001, batch_size=64\n",
      "[Epoch 1/3] Train Loss: 0.2708, Train Acc: 0.9207 | Val Loss: 0.1317, Val Acc: 0.9593\n",
      "[Epoch 2/3] Train Loss: 0.1139, Train Acc: 0.9654 | Val Loss: 0.0974, Val Acc: 0.9715\n",
      "[Epoch 3/3] Train Loss: 0.0794, Train Acc: 0.9748 | Val Loss: 0.1138, Val Acc: 0.9647\n",
      "Tiempo de entrenamiento: 64.90 seg\n",
      "\n",
      "\n",
      "Entrenando MLP con config: [128, 64], act=relu, dropout=0.2, opt=sgd, lr=0.001, batch_size=64\n",
      "[Epoch 1/3] Train Loss: 1.0137, Train Acc: 0.7016 | Val Loss: 0.3722, Val Acc: 0.8983\n",
      "[Epoch 2/3] Train Loss: 0.4242, Train Acc: 0.8755 | Val Loss: 0.2757, Val Acc: 0.9189\n",
      "[Epoch 3/3] Train Loss: 0.3358, Train Acc: 0.9026 | Val Loss: 0.2283, Val Acc: 0.9321\n",
      "Tiempo de entrenamiento: 69.94 seg\n",
      "\n",
      "\n",
      "Entrenando MLP con config: [128, 64], act=relu, dropout=0.2, opt=adam, lr=0.001, batch_size=64\n",
      "[Epoch 1/3] Train Loss: 0.3520, Train Acc: 0.8935 | Val Loss: 0.1380, Val Acc: 0.9570\n",
      "[Epoch 2/3] Train Loss: 0.1689, Train Acc: 0.9493 | Val Loss: 0.1066, Val Acc: 0.9684\n",
      "[Epoch 3/3] Train Loss: 0.1315, Train Acc: 0.9602 | Val Loss: 0.0958, Val Acc: 0.9708\n",
      "Tiempo de entrenamiento: 68.81 seg\n",
      "\n",
      "Mejor configuración MLP encontrada:\n",
      "  hidden_layers=[128, 64], activation=relu, dropout=0.2, optimizer=adam,   lr=0.001, batch_size=64, best_val_acc=0.9708, time=68.81s\n",
      "\n",
      "Entrenando CNN con config: filtros=[32, 64], fc=[128], dropout=0.0, opt=sgd, lr=0.001, batch_size=64\n",
      "[Epoch 1/3] Train Loss: 0.5691, Train Acc: 0.8423 | Val Loss: 0.1958, Val Acc: 0.9410\n",
      "[Epoch 2/3] Train Loss: 0.1582, Train Acc: 0.9532 | Val Loss: 0.1184, Val Acc: 0.9625\n",
      "[Epoch 3/3] Train Loss: 0.1008, Train Acc: 0.9700 | Val Loss: 0.0837, Val Acc: 0.9753\n",
      "Tiempo de entrenamiento: 192.52 seg\n",
      "\n",
      "\n",
      "Entrenando CNN con config: filtros=[32, 64], fc=[128], dropout=0.0, opt=adam, lr=0.001, batch_size=64\n",
      "[Epoch 1/3] Train Loss: 0.1360, Train Acc: 0.9581 | Val Loss: 0.0381, Val Acc: 0.9875\n",
      "[Epoch 2/3] Train Loss: 0.0420, Train Acc: 0.9867 | Val Loss: 0.0395, Val Acc: 0.9871\n",
      "[Epoch 3/3] Train Loss: 0.0286, Train Acc: 0.9911 | Val Loss: 0.0303, Val Acc: 0.9898\n",
      "Tiempo de entrenamiento: 180.03 seg\n",
      "\n",
      "\n",
      "Entrenando CNN con config: filtros=[64, 128], fc=[128], dropout=0.0, opt=sgd, lr=0.001, batch_size=64\n",
      "[Epoch 1/3] Train Loss: 0.4442, Train Acc: 0.8799 | Val Loss: 0.1640, Val Acc: 0.9529\n",
      "[Epoch 2/3] Train Loss: 0.1304, Train Acc: 0.9614 | Val Loss: 0.0888, Val Acc: 0.9735\n",
      "[Epoch 3/3] Train Loss: 0.0879, Train Acc: 0.9740 | Val Loss: 0.0665, Val Acc: 0.9804\n",
      "Tiempo de entrenamiento: 369.55 seg\n",
      "\n",
      "\n",
      "Entrenando CNN con config: filtros=[64, 128], fc=[128], dropout=0.0, opt=adam, lr=0.001, batch_size=64\n",
      "[Epoch 1/3] Train Loss: 0.1183, Train Acc: 0.9636 | Val Loss: 0.0410, Val Acc: 0.9861\n",
      "[Epoch 2/3] Train Loss: 0.0401, Train Acc: 0.9871 | Val Loss: 0.0353, Val Acc: 0.9883\n",
      "[Epoch 3/3] Train Loss: 0.0267, Train Acc: 0.9915 | Val Loss: 0.0290, Val Acc: 0.9891\n",
      "Tiempo de entrenamiento: 399.08 seg\n",
      "\n",
      "Mejor configuración CNN encontrada:\n",
      "  filtros=[32, 64], fc=[128], dropout=0.0, optimizer=adam,   lr=0.001, batch_size=64, best_val_acc=0.9898, time=180.03s\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # --------------------------------------------------\n",
    "    # 7.1 Experimentación con MLP\n",
    "    # --------------------------------------------------\n",
    "    hidden_layers_list = [[128, 64]]\n",
    "    activations = ['relu']\n",
    "    dropouts = [0.0, 0.2]\n",
    "    optimizers_list = ['sgd', 'adam']\n",
    "    lrs = [1e-3]\n",
    "    batch_sizes = [64]\n",
    "    num_epochs = 3  \n",
    "    \n",
    "    run_mlp_experiment(\n",
    "        hidden_layers_list=hidden_layers_list,\n",
    "        activations=activations,\n",
    "        dropouts=dropouts,\n",
    "        optimizers=optimizers_list,\n",
    "        lrs=lrs,\n",
    "        batch_sizes=batch_sizes,\n",
    "        num_epochs=num_epochs\n",
    "    )\n",
    "    \n",
    "    # --------------------------------------------------\n",
    "    # 7.2 Experimentación con CNN\n",
    "    # --------------------------------------------------\n",
    "    filters_list = [\n",
    "        [32, 64],\n",
    "        [64, 128]\n",
    "    ]\n",
    "    fc_sizes_list = [\n",
    "        [128]\n",
    "    ]\n",
    "    dropouts = [0.0]\n",
    "    \n",
    "    run_cnn_experiment(\n",
    "        filters_list=filters_list,\n",
    "        fc_sizes_list=fc_sizes_list,\n",
    "        dropouts=dropouts,\n",
    "        optimizers=optimizers_list,\n",
    "        lrs=lrs,\n",
    "        batch_sizes=batch_sizes,\n",
    "        num_epochs=num_epochs\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **RESULTADOS COMPLETOS (EN CASO DE QUE NO SE VEA ARRIBA)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenando MLP con config: [128, 64], act=relu, dropout=0.0, opt=sgd, lr=0.001, batch_size=64\n",
    "[Epoch 1/3] Train Loss: 0.8595, Train Acc: 0.7761 | Val Loss: 0.3468, Val Acc: 0.9009\n",
    "[Epoch 2/3] Train Loss: 0.3152, Train Acc: 0.9091 | Val Loss: 0.2679, Val Acc: 0.9233\n",
    "[Epoch 3/3] Train Loss: 0.2577, Train Acc: 0.9254 | Val Loss: 0.2284, Val Acc: 0.9334\n",
    "Tiempo de entrenamiento: 78.87 seg\n",
    "\n",
    "\n",
    "Entrenando MLP con config: [128, 64], act=relu, dropout=0.0, opt=adam, lr=0.001, batch_size=64\n",
    "[Epoch 1/3] Train Loss: 0.2708, Train Acc: 0.9207 | Val Loss: 0.1317, Val Acc: 0.9593\n",
    "[Epoch 2/3] Train Loss: 0.1139, Train Acc: 0.9654 | Val Loss: 0.0974, Val Acc: 0.9715\n",
    "[Epoch 3/3] Train Loss: 0.0794, Train Acc: 0.9748 | Val Loss: 0.1138, Val Acc: 0.9647\n",
    "Tiempo de entrenamiento: 64.90 seg\n",
    "\n",
    "\n",
    "Entrenando MLP con config: [128, 64], act=relu, dropout=0.2, opt=sgd, lr=0.001, batch_size=64\n",
    "[Epoch 1/3] Train Loss: 1.0137, Train Acc: 0.7016 | Val Loss: 0.3722, Val Acc: 0.8983\n",
    "[Epoch 2/3] Train Loss: 0.4242, Train Acc: 0.8755 | Val Loss: 0.2757, Val Acc: 0.9189\n",
    "[Epoch 3/3] Train Loss: 0.3358, Train Acc: 0.9026 | Val Loss: 0.2283, Val Acc: 0.9321\n",
    "Tiempo de entrenamiento: 69.94 seg\n",
    "\n",
    "\n",
    "Entrenando MLP con config: [128, 64], act=relu, dropout=0.2, opt=adam, lr=0.001, batch_size=64\n",
    "[Epoch 1/3] Train Loss: 0.3520, Train Acc: 0.8935 | Val Loss: 0.1380, Val Acc: 0.9570\n",
    "[Epoch 2/3] Train Loss: 0.1689, Train Acc: 0.9493 | Val Loss: 0.1066, Val Acc: 0.9684\n",
    "[Epoch 3/3] Train Loss: 0.1315, Train Acc: 0.9602 | Val Loss: 0.0958, Val Acc: 0.9708\n",
    "Tiempo de entrenamiento: 68.81 seg\n",
    "\n",
    "Mejor configuración MLP encontrada:\n",
    "  hidden_layers=[128, 64], activation=relu, dropout=0.2, optimizer=adam,   lr=0.001, batch_size=64, best_val_acc=0.9708, time=68.81s\n",
    "\n",
    "Entrenando CNN con config: filtros=[32, 64], fc=[128], dropout=0.0, opt=sgd, lr=0.001, batch_size=64\n",
    "[Epoch 1/3] Train Loss: 0.5691, Train Acc: 0.8423 | Val Loss: 0.1958, Val Acc: 0.9410\n",
    "[Epoch 2/3] Train Loss: 0.1582, Train Acc: 0.9532 | Val Loss: 0.1184, Val Acc: 0.9625\n",
    "[Epoch 3/3] Train Loss: 0.1008, Train Acc: 0.9700 | Val Loss: 0.0837, Val Acc: 0.9753\n",
    "Tiempo de entrenamiento: 192.52 seg\n",
    "\n",
    "\n",
    "Entrenando CNN con config: filtros=[32, 64], fc=[128], dropout=0.0, opt=adam, lr=0.001, batch_size=64\n",
    "[Epoch 1/3] Train Loss: 0.1360, Train Acc: 0.9581 | Val Loss: 0.0381, Val Acc: 0.9875\n",
    "[Epoch 2/3] Train Loss: 0.0420, Train Acc: 0.9867 | Val Loss: 0.0395, Val Acc: 0.9871\n",
    "[Epoch 3/3] Train Loss: 0.0286, Train Acc: 0.9911 | Val Loss: 0.0303, Val Acc: 0.9898\n",
    "Tiempo de entrenamiento: 180.03 seg\n",
    "\n",
    "\n",
    "Entrenando CNN con config: filtros=[64, 128], fc=[128], dropout=0.0, opt=sgd, lr=0.001, batch_size=64\n",
    "[Epoch 1/3] Train Loss: 0.4442, Train Acc: 0.8799 | Val Loss: 0.1640, Val Acc: 0.9529\n",
    "[Epoch 2/3] Train Loss: 0.1304, Train Acc: 0.9614 | Val Loss: 0.0888, Val Acc: 0.9735\n",
    "[Epoch 3/3] Train Loss: 0.0879, Train Acc: 0.9740 | Val Loss: 0.0665, Val Acc: 0.9804\n",
    "Tiempo de entrenamiento: 369.55 seg\n",
    "\n",
    "\n",
    "Entrenando CNN con config: filtros=[64, 128], fc=[128], dropout=0.0, opt=adam, lr=0.001, batch_size=64\n",
    "[Epoch 1/3] Train Loss: 0.1183, Train Acc: 0.9636 | Val Loss: 0.0410, Val Acc: 0.9861\n",
    "[Epoch 2/3] Train Loss: 0.0401, Train Acc: 0.9871 | Val Loss: 0.0353, Val Acc: 0.9883\n",
    "[Epoch 3/3] Train Loss: 0.0267, Train Acc: 0.9915 | Val Loss: 0.0290, Val Acc: 0.9891\n",
    "Tiempo de entrenamiento: 399.08 seg\n",
    "\n",
    "Mejor configuración CNN encontrada:\n",
    "  filtros=[32, 64], fc=[128], dropout=0.0, optimizer=adam,   lr=0.001, batch_size=64, best_val_acc=0.9898, time=180.03s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **MLP (Perceptrón Multicapa):**\n",
    "Mejor configuración encontrada:\n",
    "\n",
    "hidden_layers: [128, 64]\n",
    "activation: relu\n",
    "dropout: 0.2\n",
    "optimizer: adam\n",
    "learning_rate: 0.001\n",
    "batch_size: 64\n",
    "Validation Accuracy: 97.08%\n",
    "Training Time: 68.81 segundos\n",
    "Observaciones:\n",
    "\n",
    "* La inclusión de dropout (0.2) con el optimizador Adam ayudó a mejorar la capacidad de generalización del modelo, alcanzando una alta precisión en validación con una penalización mínima en tiempo de entrenamiento.\n",
    "* Adam mostró un mejor desempeño comparado con SGD debido a su capacidad de ajustar la tasa de aprendizaje dinámicamente durante el entrenamiento.\n",
    "* La configuración con dropout=0.2 manejó mejor el sobreajuste, logrando mejores resultados en validación que con dropout=0.0.\n",
    "\n",
    "##### **CNN (Red Neuronal Convolucional):**\n",
    "Mejor configuración encontrada:\n",
    "\n",
    "filtros: [32, 64]\n",
    "fc_layers: [128]\n",
    "dropout: 0.0\n",
    "optimizer: adam\n",
    "learning_rate: 0.001\n",
    "batch_size: 64\n",
    "Validation Accuracy: 98.98%\n",
    "Training Time: 180.03 segundos\n",
    "Observaciones:\n",
    "\n",
    "* La arquitectura más simple ([32, 64] filtros) combinada con Adam permitió un tiempo de entrenamiento razonable y alcanzó una validación significativamente alta (98.98%).\n",
    "* Al aumentar la cantidad de filtros ([64, 128]), aunque se logró una leve mejora en entrenamiento, no se tradujo en una validación significativamente mejor, pero el tiempo de entrenamiento aumentó considerablemente.\n",
    "* Adam superó nuevamente a SGD debido a su eficiencia en la optimización para arquitecturas más profundas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Comparación MLP vs. CNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Precisión:**\n",
    "\n",
    "* CNN supera al MLP en validación por un margen de aproximadamente 1.9%, lo cual es significativo en tareas con imágenes o datos estructurados.\n",
    "\n",
    "##### **Tiempo de Entrenamiento:**\n",
    "\n",
    "* El MLP tiene una ventaja significativa en tiempo de entrenamiento, siendo 2.6 veces más rápido que la CNN.\n",
    "* Esto sugiere que para casos donde el tiempo es crítico y una precisión ligeramente menor es aceptable, MLP podría ser preferible.\n",
    "\n",
    "##### **Robustez y Complejidad:**\n",
    "\n",
    "* CNN maneja características espaciales mejor que MLP, lo que explica su mejor desempeño en validación.\n",
    "* MLP es más simple y menos costoso computacionalmente, lo que lo hace más adecuado para conjuntos de datos más pequeños o tabulares."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
